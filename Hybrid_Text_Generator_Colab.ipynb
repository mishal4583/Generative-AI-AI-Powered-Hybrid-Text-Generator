{"cells": [{"cell_type": "markdown", "metadata": {}, "source": ["# \ud83e\udde0 Hybrid Text Generator (Markov + GPT-2)\n", "**Creative text generation using statistical and neural NLP methods.**\n", "- Markov Chains (for structure & grammar)\n", "- GPT-2 (for coherence & creativity)\n", "- Gradio UI\n", "- GPU-accelerated (Colab + HuggingFace)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2705 Install required libraries\n", "!pip install markovify gradio transformers textstat matplotlib"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcc2 Upload your cleaned dataset files here (pride_sentences.txt, frankenstein_sentences.txt, etc.)\n", "from google.colab import files\n", "uploaded = files.upload()"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\udcda Load Markovify models from cleaned text\n", "import markovify\n", "\n", "with open(\"pride_sentences.txt\") as f:\n", "    text1 = f.read()\n", "with open(\"frankenstein_sentences.txt\") as f:\n", "    text2 = f.read()\n", "\n", "model1 = markovify.Text(text1, state_size=3)\n", "model2 = markovify.Text(text2, state_size=3)\n", "\n", "combined_model = markovify.combine([model1, model2], [1, 1])"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83e\udd16 Load GPT-2 Model (HuggingFace Transformers)\n", "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n", "import torch\n", "\n", "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n", "tokenizer = GPT2Tokenizer.from_pretrained('distilgpt2')\n", "model = GPT2LMHeadModel.from_pretrained('distilgpt2').to(device)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \u2728 GPT-2 Text Generation Function\n", "def refine_with_gpt2(prompt, max_new_tokens=100, temperature=0.7, top_p=0.9):\n", "    inputs = tokenizer(prompt, return_tensors='pt').to(device)\n", "    outputs = model.generate(**inputs, max_new_tokens=max_new_tokens, \n", "                             temperature=temperature, top_p=top_p,\n", "                             pad_token_id=tokenizer.eos_token_id)\n", "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# \ud83d\ude80 Gradio UI\n", "import gradio as gr\n", "\n", "def hybrid_generate(user_prompt, max_tokens, temp, top_p):\n", "    markov_prompt = user_prompt.strip() if user_prompt else combined_model.make_short_sentence(120)\n", "    gpt_output = refine_with_gpt2(markov_prompt, max_new_tokens=max_tokens, temperature=temp, top_p=top_p)\n", "    return markov_prompt, gpt_output\n", "\n", "demo = gr.Interface(\n", "    fn=hybrid_generate,\n", "    inputs=[\n", "        gr.Textbox(label=\"Optional Prompt (Leave empty for random Markov)\", lines=1),\n", "        gr.Slider(50, 300, value=100, step=10, label=\"Max Tokens (GPT-2)\"),\n", "        gr.Slider(0.1, 1.5, value=0.7, step=0.1, label=\"Temperature\"),\n", "        gr.Slider(0.5, 1.0, value=0.9, step=0.05, label=\"Top-p (Nucleus Sampling)\")\n", "    ],\n", "    outputs=[\n", "        gr.Textbox(label=\"Markov Prompt\"),\n", "        gr.Textbox(label=\"GPT-2 Refined Output\")\n", "    ],\n", "    title=\"Hybrid Text Generator (Markov + GPT-2)\",\n", "    description=\"Generate creative text by combining statistical and neural NLP methods.\"\n", ")\n", "\n", "demo.launch(share=True)"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"name": "python", "version": "3.8"}}, "nbformat": 4, "nbformat_minor": 2}